### Notes

#### Chapter 2: Understanding Foundational Models

- Picking out the most common output among a set of outputs can be especially
  useful for tasks that expect exact answers.
- To get a model to give structured output we can give it instructions i.e
  prompting. We can do that and some manual post-processsing where we have a
  script etc fix small mistakes. We can also do something called constrain
  sampling. This is expensive and non-trivial. Finetuning is the best bang for
  your buck approach to get the expected output. But as modles get better at
  following instructions prompting will probably be enough in the future

#### Chapter 3: Evalutation Methodology
There are a couple of different metrics used to develop language models 
- Entropy, measures how much informatio on averag a token carries. The higher
  the entropy, the more information each token carries and the more bits are
  needed to represent a token. For example say you have a languge to describe
  position in a square, it can be A or B. One bit is enough in this case to
  represent the positon in this language, thus this language has entropy of 1.
- Cross Entropy, is the measure of how difficult it is for the language model to
  predict what comes next in the dataset it was trained on. The two main
  components of cross entropy is:
  a) The training data's predictability, measures by the training data's entropy
  b) How the distribution captured by the language model diverges from the true
     distribution of the training data.
  A language models is trained to minimize it's cross entropy with respect to
  the training data. If the language model learns perfectly from it's training
  data, the models cross entropy will be exactly the same as the entropy of the
  training data. The kullback-leibler (KL) divergence will then be 0.
- Bits-per-character(BPC) and Bits-per-byte(BPB): one unit of entropy and cross
  entropy is bits. If the cross entropy of a language model is 6 bits, this
  language model needs 6 bits to represent each token.
  
  Since language models have different tokenization techniques we usually talk
  about BPC and BPB instead. If number of bits per token is 6 and on average,
  each token is 2 charachters, BCP = 6/2 = 3.
  But characthers can have different encoding schemas which makes BPC a hard
  measure to standardize. Instead with BPB we have the number of bits a language
  models needs to represent one byte of the original training data. If the BPC
  is 3 and each character is 7 bits or 7/8 of a byte, then BPB is 3/(7/8) 3.43.
  
  Cross entropy tells us how efficient a language model will be at compressing.
  For example if BPB is 3.43 it means that it can represent each byte in the
  original text with 3.43 bits. Thus it can compress the original text to less
  than half its size.
- Perplexity, if cross entropy measures how difficult it is to predict the next
  token, perplexity measure the amount of uncertainty it when predicting the
  next token. Higher uncertainty means tere are more possible options for the
  next token. In the square example if we could have 4 positons instead and thus
  needing 2 bits i.e and Entropy of 2. Then for a model to predict a position in
  that language it has 4 possibilites, thus the perplexity of that model is 4.

#### Evaluation Strategies
There is open ended evaluation, which the coming notes will be about, and there
is close ended evaluation. Generally close ended evaluation has many established
methods so search for them. For open ended exact evaluation there are 2
categories
a) Functional correctness: Let's say we ask a language model to generate a
   function like add(a,b). We can then use tests to 100% determine if what was
   generated was functionlly correct. You could ofcourse expand this functional
   correctness to more complex features etc, but then it becomes much harder to
   have 100% functionl correctness.
b) Similarity Measurements Against Reference Data: Since this evaluation method
   requires reference data it's bottlenecked by how much and how fast the
   reference data can be generated. This data is usually generated by humans but
   AI is being used more and more to also generate the reference data which is
   then reviewd by the human. There are 4 ways to measure similarity between
   models:
   1) Exact match: Whether the generated response matches one of the reference
      responses exactly. Exact matching works tasks that expect short exact
      answers such as simple math problems, common knowledge queries and trivia
      style questions.
   2) Lexical similarity: Measures how much two texts overlap. Basically this
      can be measured by counting how many tokes two texts have in common.
      Another way to measure is called fuzzy matching which checks similarity by
      "edit distance", i.e how many edits it would take to convert from one text
      another. In this same spirit sometimes "n-gram similarity" is used.
      This method is limited by the fact that it needs a very comprehensive
      reference set. Becase maybe a generated response is great but does not
      exist in the reference set, then it would get a low eval score. Another
      interesting discovery about lexical similarity is that higher scores dont
      always mean better responses. In some benchmarks researches have found
      that the correct and incorrect responses almost had the same scores.
      Indicating for example on coding benchmarks that optimizing for lexical
      similarity does not give more functionally correct responses atleast.
   3) Semantic similarity: This first requires transforming a text into a
      numerical representation called an embedding. Then the similarity between
      two embeddings can be computed using metrics such as cosine similiarity.
      Two embeddings that are the same have a score of 1. Two opposite
      embeddings have a score of -1. This measure does not require a
      comprehensive reference set. However it is highly dependant on the quality
      of the embedding algorithm used. Another drawback is that the embedding
      algorithm can require non-trivial compute and time to run.
   4) AI as a Judge: Essentially this should be used as a fallback option. It
      becomes even harder to justify if the application has strict latency
      requirments since the number of api call to models will be higher. But if
      there is a need to use AI as a judge the prompts follow the same pattern/
      best practices as any other prompts:
      1) The task the models is to perform.
      2) The criteria the model should follow to evaluate. The more detailed
	 instruction the better.
      3) The scoring system:
	      a) Classificaton, such as good/bad.
	      b) Discrete numbers 1 to 5.
	      c) Continous numerical values 0 to 1.
      In general classification works best with LLM's and then discret numbers

#### Limitations of AI as a Judge
- Self-bias: A model tends to favour its own responses.
- First positon bias: An AI judge may favour the first answer in a pairwise
  comparison or the first list item in a list of options.
- Verbosity bias: Some AI judges favour lengthier answers. regardless of the
  quality. This particular bias has been decreasing as models have become
  better.

#### What Models Can Act as Judges?
Having a stronger model evaluate a subset of answers generated by weaker cheaper
models is a valid strat for keeping costs lower. The stronger model couls also
be too slow. So a fast model is used to generate responses and the stronger one
does evaluation in the background. The reverse of this is also a valid strag.
For judging there is a new research directon of having small specialised judges
such as Cappy, BLUERT and pandaLM

#### Ranking Modles With Comparative Evaluation
We have comparative evaluation and pointwise evaluation. Pointwise evaluation is
to take each response from different models, score them, then in the end pick
the one with the highest score. Comparative evaluation on the other hand is
based on looking at all the responses from the different models at the same time
and then picking the best one. This is grea for responses where user preference
is the target. Note however that not all questions should be answered by
preference, e.g facts questions etc.

Comparative eal shouldn't be confused with A/B testing. In A/B testing a user
sees the output of one of the models at a time, in comparative evaluation they
see outputs form multiple models at the same time.

For comparative evaluation each round of evaluation is caleed a match. The
probability that model A is prefered over B is called the winrate of A over B.
This is the perfect environment for a ranking algorithm, these are new in AI but
been there for a long time in sports and games. Algorithms such as Elo,
Bradley-Terry and TrueSkill to name a few. Through this lens we can see that
model ranking is then a predicitve problem. We compute a rank from historical
match outcomes and use it to predict future match outcomes.

One important aspect of comparative evaluation that should not be forgotten is
that comparative evaluation tells us which models is better. It does not tell us
how good a model is or whether this model is good for our use case.


---
status: :ðŸ“–:
tags: [[Book notes]] - [[030 Software Development.md]] - [[001 Book Index.md]]
date: 2025-04-18
